{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc7ef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pip install pandas \n",
    "pip install scipy\n",
    "pip install matplotlib\n",
    "pip install ipython\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcb341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import periodogram\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06217b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading dataset file into a dataframe called 'data'\n",
    "data = pd.read_csv('./Sub_Division_IMD_2017.csv')\n",
    "display(data)    #  Displaying the DataFrame that we just read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94fa2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Exploration:\n",
    "# After inspecting the data, I see that the data is of 117 years of only some selected states in India, with their rainfall data for each month.\n",
    "# The data ranges from 1901-2017. With a data of 100+ years, we can analyze various aspects.\n",
    "\n",
    "# Analytical Goals:\n",
    "# 1. Long-term trend\n",
    "# 2. Seasonal Pattern\n",
    "# 3. Months with max-min rainfall and variability\n",
    "# 4. Extreme events in our centurial data\n",
    "# 5. State-wise analysis\n",
    "# 6. Trend-line (direction and pattern of my data)\n",
    "# 7. Cyclic patterns\n",
    "# 8. Comparing two different months of the same and different years\n",
    "# 9. Time-series of the state with max-min rainfall and understanding the pattern for the same\n",
    "# 10. Spatial Analysis\n",
    "# 11. Forecast the future rainfall patterns\n",
    "# 12. Grouping the data into groups of 15-20 years and analyzing the variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de90f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = data.dropna()\n",
    "\n",
    "# Compute the periodogram\n",
    "freq, power = periodogram(df['ANNUAL'], fs=1, scaling='spectrum', nfft=1024)\n",
    "\n",
    "# Plot the periodogram\n",
    "plt.plot(1 / freq, power)\n",
    "plt.xlabel('Period (Years)')\n",
    "plt.ylabel('Power Spectral Density')\n",
    "plt.title('Periodogram of Annual Rainfall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e1d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.dropna()\n",
    "\n",
    "# Compute the periodogram\n",
    "freq, power = periodogram(df['ANNUAL'], fs=1, scaling='spectrum', nfft=1024)\n",
    "\n",
    "# Check and handling division by zero\n",
    "# to resolve the warning\n",
    "non_zero_freq = freq.copy()\n",
    "non_zero_freq[non_zero_freq == 1] = 1e-10  \n",
    "\n",
    "# Plot the periodogram\n",
    "plt.plot(1 / non_zero_freq, power)\n",
    "plt.xlabel('Period (Years)')\n",
    "plt.ylabel('Power Spectral Density')\n",
    "plt.title('Periodogram of Annual Rainfall')\n",
    "plt.show()\n",
    "\n",
    "# I'm able to plot the graph but unable to res was not able to resolve the warning, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855a9f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# omitting last 4 columns (JF,MAM,JJA, OND)- seasonal analysis\n",
    "data1 = data.iloc[:, :-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f7a7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Uttarakhand = (data1 == 'Uttarakhand').any(axis=1)\n",
    "Uttarakhand = data1[Uttarakhand]\n",
    "\n",
    "# Statistical results\n",
    "mean_value = np.mean(Uttarakhand['ANNUAL'])\n",
    "print(mean_value)\n",
    "median_value = np.median(Uttarakhand['ANNUAL'])\n",
    "print(median_value)\n",
    "\n",
    "std_dev = np.std(Uttarakhand['ANNUAL'])\n",
    "print(std_dev)\n",
    "variance = np.var(Uttarakhand['ANNUAL'])\n",
    "print(variance)\n",
    "\n",
    "skewness = Uttarakhand['ANNUAL'].skew()\n",
    "print(skewness)\n",
    "kurtosis = Uttarakhand['ANNUAL'].kurt()\n",
    "print(kurtosis)\n",
    "\n",
    "minimum = np.min(Uttarakhand['ANNUAL'])\n",
    "print(minimum)\n",
    "maximum = np.max(Uttarakhand['ANNUAL'])\n",
    "print(maximum)\n",
    "\n",
    "q1 = np.percentile(Uttarakhand['ANNUAL'] , 25)\n",
    "print(q1)\n",
    "q3 = np.percentile(Uttarakhand['ANNUAL'] , 75)\n",
    "print(q3)\n",
    "\n",
    "iqr = q3 - q1\n",
    "print(iqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3510cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a box plot for all columns\n",
    "data1.boxplot(rot=45, figsize=(12, 8))\n",
    "plt.title('Box Plot for Each Column')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Box-plot( box-and-whisker plot) helps us to visualize the distribution of data\n",
    "# helps visualize the spread, skewness, and central tendency of the data\n",
    "# We can see outliers in our box-plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b540f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Outlier rejection\n",
    "\n",
    "# Set a threshold for Z-scores (e.g., 3)\n",
    "threshold = 3\n",
    "\n",
    "# Create an empty DataFrame to store outliers\n",
    "outliers_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each numeric column\n",
    "for column_name in data1.select_dtypes(include=np.number).columns:\n",
    "    # Calculate Z-scores\n",
    "    z_scores = np.abs((data1[column_name] - data1[column_name].mean()) / data1[column_name].std())\n",
    "\n",
    "    # Identify outliers\n",
    "    column_outliers = data1[z_scores > threshold]\n",
    "\n",
    "    # Append outliers to the outliers_df\n",
    "    outliers_df = pd.concat([outliers_df, column_outliers])\n",
    "\n",
    "# Remove duplicates from outliers_df\n",
    "outliers_df = outliers_df.drop_duplicates()\n",
    "\n",
    "# Remove outliers from the original DataFrame\n",
    "df_cleaned = data1.drop(outliers_df.index)\n",
    "\n",
    "# Display information about removed outliers\n",
    "print(f'Number of outliers removed: {len(outliers_df)}')\n",
    "print('Outliers:')\n",
    "print(outliers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c778f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a box plot for all columns\n",
    "df_cleaned.boxplot(rot=45, figsize=(12, 8))\n",
    "plt.title('Box Plot for Each Column')\n",
    "plt.show()\n",
    "\n",
    "# Box-plot here has no outliers, will be using the clean data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e445e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Seasonal Analysis\n",
    "\n",
    "# Reading the set again , In this analysis I'm using the last 4 columns of my set\n",
    "# I'm plotting months with time \n",
    "df = pd.read_csv(\"./Sub_Division_IMD_2017.csv\")\n",
    "df = df.dropna()\n",
    "\n",
    "# as i have more than 20 states, choosing single state and analyzing which seasons have max-min rainfall and variability\n",
    "# JF  - Jan and feb\n",
    "# MAM - mar, april and may\n",
    "# JJAS - june, june , august and september\n",
    "# OND - october, november and december \n",
    "seasons = (df == 'Uttarakhand').any(axis=1)\n",
    "seasons = df[seasons]\n",
    "\n",
    "# Omitting the rest of the columns\n",
    "seasons =  seasons.iloc[:, :2].join(seasons.iloc[:, -4:])\n",
    "seasons.plot( x = 'YEAR' , figsize=(30,10), linestyle = '--' , marker = '.')\n",
    "\n",
    "# the graph shows JUNE, JULY, AUGUST AND SEPT are the months with max rainfall\n",
    "# we can conclude JJAS - rainy season in india(uttarakhand)\n",
    "# we can check for all the states similary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f36e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal patterns\n",
    "\"\"\"\n",
    "To find seasonal patterns in a rainfall dataset spanning 100 years, I am using time series analysis\n",
    "seasonal decomposition, which separates the data into its underlying components:\n",
    "trend, seasonal, and residual.\n",
    "Trend will help us look for long-term patterns or trends in the data, if it is increasing, decreasing, or relatively stable over time?\n",
    "Seasonal helps us Identify repeating patterns that occur at regular intervals, seasonality can be noted with the peaks and trough\n",
    "Residual Checks for any remaining patterns or irregularities in the data.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "Uttarakhand_cleaned = (df_cleaned == 'Uttarakhand').any(axis=1)\n",
    "Uttarakhand_cleaned = df_cleaned[Uttarakhand_cleaned]\n",
    "print(Uttarakhand_cleaned)\n",
    "\n",
    "decomposition = seasonal_decompose(Uttarakhand_cleaned['ANNUAL'], model = 'multiplicative', period = 12)\n",
    "plt.figure(figsize = (20,8))\n",
    "plt.subplot(4,1,1)\n",
    "plt.plot(Uttarakhand_cleaned.index, decomposition.trend, label='Trend', color = 'blue')\n",
    "plt.legend()\n",
    "plt.subplot(4,1,2)\n",
    "plt.plot(Uttarakhand_cleaned.index, decomposition.seasonal, label='seasonal', color = 'green')\n",
    "plt.legend()\n",
    "plt.subplot(4,1,3)\n",
    "plt.plot(Uttarakhand_cleaned.index, decomposition.resid, label='residual', color = 'red')\n",
    "plt.legend()\n",
    "plt.subplot(4,1,4)\n",
    "plt.plot(Uttarakhand_cleaned.index, Uttarakhand_cleaned['ANNUAL'], label='original', color = 'black')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeccf09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "running average or moving average\n",
    "Smoothing the data using running average or moving averageto to reduce short-term variability and highlight long term trends.\n",
    "SMA AND EMA\n",
    "\"\"\"\n",
    "\n",
    "# simple Moving averages\n",
    "\n",
    "Uttarakhand_cleaned['Simple Moving Average'] = Uttarakhand_cleaned['ANNUAL'].rolling(window = window_size).mean()\n",
    "\n",
    "plt.plot(Uttarakhand_cleaned.index , Uttarakhand_cleaned['ANNUAL'], label = 'Original', color = 'blue')\n",
    "plt.plot(Uttarakhand_cleaned.index , Uttarakhand_cleaned['Simple Moving Average'], label = '12 month Moving average', color = 'red')\n",
    "\n",
    "# exponential Moving averages\n",
    "\n",
    "window_size = 10\n",
    "Uttarakhand_cleaned['Exponentially Moving Average'] = Uttarakhand_cleaned['ANNUAL'].ewm(span = window_size , adjust = False ).mean()\n",
    "\n",
    "plt.plot(Uttarakhand_cleaned.index , Uttarakhand_cleaned['ANNUAL'], label = 'Original', color = 'blue')\n",
    "plt.plot(Uttarakhand_cleaned.index , Uttarakhand_cleaned['Exponentially Moving Average'], label = '12 month Moving average', color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c2c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trend line\n",
    "# Trend line signifies the general direction of the pattern of our data.\n",
    "\n",
    "x = Uttarakhand_cleaned['YEAR']\n",
    "y = Uttarakhand_cleaned['ANNUAL']\n",
    "\n",
    "coefficients = np.polyfit(x,y,1)\n",
    "polyfit_line = np.poly1d(coefficients)\n",
    "\n",
    "fit_y = polyfit_line(x)\n",
    "\n",
    "plt.figure(figsize = (25,6))\n",
    "plt.scatter(x,y,label = 'Data Points')\n",
    "plt.plot(x, fit_y, color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5869f193",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merging = df_cleaned.iloc[:, :15]\n",
    "df_merging \n",
    "\n",
    "# saving the cleaned data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aae57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.iloc[:, :14]\n",
    "\n",
    "# Setting States and Year as indices and then converting the dataframe into long format using stack() method\n",
    "df_cleaned = df_cleaned.set_index(['YEAR', 'CITY']).stack().reset_index()\n",
    "\n",
    "# Defining mapper dictionary to change the column names of last 2 columns\n",
    "mapper = {df_cleaned.columns.values[2]:'MONTHS',\n",
    "          df_cleaned.columns.values[3]:'RAINFALL IN MM'}\n",
    "\n",
    "# Renaming the columns\n",
    "df_cleaned.rename(columns=mapper, inplace=True)\n",
    "\n",
    "# Displaying the dataframe \n",
    "display(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95461a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Temporal Analysis - Time Series of Yearly Rainfall\n",
    "yearly_rainfall = df_cleaned.groupby('YEAR')['RAINFALL IN MM'].sum()\n",
    "yearly_rainfall.plot(kind='line', marker='o', title='Yearly Rainfall Trend')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Total Rainfall (mm)')\n",
    "plt.show()\n",
    "\n",
    "# 2. Seasonal Patterns - Average Rainfall for Each Month\n",
    "monthly_average = df_cleaned.groupby('MONTHS')['RAINFALL IN MM'].mean()\n",
    "monthly_average.plot(kind='bar', title='Average Monthly Rainfall')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Rainfall (mm)')\n",
    "plt.show()\n",
    "\n",
    "# 3. Monthly Distribution - Box Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot([df_cleaned[df_cleaned['MONTHS'] == month]['RAINFALL IN MM'] for month in df_cleaned['MONTHS'].unique()],\n",
    "            labels=df_cleaned['MONTHS'].unique())\n",
    "plt.title('Monthly Rainfall Distribution')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Rainfall (mm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19ce3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the data is of 117 years, lets do some grouped-analysis\n",
    "# I'll making groups of 15 years, and comparing the groups\n",
    "\n",
    "# re-reading the data\n",
    "df = pd.read_csv(\"./Sub_Division_IMD_2017.csv\")\n",
    "\n",
    "\n",
    "def create_year_groups(dataframe, group_size):\n",
    "    groups = []\n",
    "    for start_year in range(dataframe['YEAR'].min(), dataframe['YEAR'].max() + 1, group_size):\n",
    "        end_year = start_year + group_size - 1\n",
    "        group = dataframe[(dataframe['YEAR'] >= start_year) & (dataframe['YEAR'] <= end_year)]\n",
    "        groups.append(group)\n",
    "    return groups\n",
    "\n",
    "\n",
    "groups_of_15_years = create_year_groups(df, 15)\n",
    "\n",
    "\n",
    "for i, group in enumerate(groups_of_15_years):\n",
    "    max_rainfall_year = group.loc[group['ANNUAL'].idxmax()]['YEAR']\n",
    "    print(f\"Group {i + 1}: Maximum Rainfall Year - {max_rainfall_year}\")\n",
    "\n",
    "# Maximum rainfall states \n",
    "max_rainfall_states_overall = df.loc[df.groupby('CITY')['ANNUAL'].idxmax()][['CITY', 'YEAR', 'ANNUAL']]\n",
    "print(\"\\nMaximum Rainfall States Overall:\")\n",
    "print(max_rainfall_states_overall)\n",
    "\n",
    "# Minimum rainfall \n",
    "min_rainfall_group = min(groups_of_15_years, key=lambda x: x['ANNUAL'].sum())\n",
    "print(\"\\nMinimum Rainfall Among the Groups:\")\n",
    "print(min_rainfall_group[['YEAR', 'ANNUAL']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814969a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial Analysis\n",
    "\n",
    "main_data = df_merging\n",
    "# Loading the dataset containing latitude and longitude\n",
    "# 2nd dataset, will be merging both the datasets for spatial analysis\n",
    "lat_lon_data = pd.read_csv('./poptable.csv')\n",
    "\n",
    "# Check the columns in each DataFrame\n",
    "print(\"Columns in main_data:\", main_data.columns)\n",
    "print(\"Columns in lat_lon_data:\", lat_lon_data.columns)\n",
    "\n",
    "lat_lon_data['CITY'] = lat_lon_data['CITY'].str.strip()\n",
    "main_data['CITY'] = main_data['CITY'].str.strip()\n",
    "\n",
    "lat_lon_data.dropna(subset=['CITY'], inplace=True)\n",
    "main_data.dropna(subset=['CITY'], inplace=True)\n",
    "\n",
    "lat_lon_data['CITY'] = lat_lon_data['CITY'].astype(str)\n",
    "main_data['CITY'] = main_data['CITY'].astype(str)\n",
    "\n",
    "lat_lon_data['CITY'] = lat_lon_data['CITY'].str.replace('[^\\w\\s]', '')\n",
    "main_data['CITY'] = main_data['CITY'].str.replace('[^\\w\\s]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe9c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all string columns to uppercase\n",
    "df_merging['CITY'] = df_merging['CITY'].str.upper()  # Example for a specific column\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df_merging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5184386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the datasets based on the 'CITY' column\n",
    "merged_data = pd.merge(df_merging, lat_lon_data[['CITY', 'LATITUDE', 'LONGITUDE']], on='CITY', how='left')\n",
    "\n",
    "# Display the merged dataset\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ada9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data.dropna()\n",
    "# dropping the null values\n",
    "merged_data\n",
    "# displaying the merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6df3fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(merged_data, geometry=gpd.points_from_xy(merged_data[\"LONGITUDE\"], merged_data[\"LATITUDE\"]))\n",
    "\n",
    "# Create a choropleth map\n",
    "gdf.plot(column=\"ANNUAL\", cmap=\"viridis\", legend=True)\n",
    "plt.title(\"Spatial Distribution of Annual Rainfall\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cebb384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "\n",
    "merged_data = merged_data.drop_duplicates().dropna()\n",
    "# Create a geolocator\n",
    "geolocator = Nominatim(user_agent=\"state_geocoding\")\n",
    "\n",
    "# Geocode each state and add latitude and longitude to the DataFrame\n",
    "merged_data['location'] = merged_data['CITY'].apply(geolocator.geocode)\n",
    "merged_data['LATITUDE'] = merged_data['location'].apply(lambda loc: loc.latitude if loc else None)\n",
    "merged_data['LONGITUDE'] = merged_data['location'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "merged_data_unique_city = merged_data_unique_city.drop(['location'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d479bc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scatter plot for two specific months\n",
    "month1 = \"JAN\"\n",
    "month2 = \"JUL\"\n",
    "plt.scatter(data[month1], data[month2])\n",
    "plt.xlabel(month1)\n",
    "plt.ylabel(month2)\n",
    "plt.title(\"Correlation between \" + month1 + \" and \" + month2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
